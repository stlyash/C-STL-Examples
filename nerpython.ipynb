{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAhl6mVjJA/jHqMZy+Q7T1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stlyash/Cpp-STL-Examples/blob/main/nerpython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "qhin5twXqOIV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51_k3UbkjE_y"
      },
      "outputs": [],
      "source": [
        "!pip install spacy_transformers\n",
        "\n",
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import spacy_transformers\n",
        "from spacy.tokens import DocBin\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "metadata": {
        "id": "8Knc4hJNlqWQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stlyash/spacyResumeParcer.git"
      ],
      "metadata": {
        "id": "NLzmkNBmmBeD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4113a373-0a17-4d85-bdb4-80c70682ea47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'spacyResumeParcer'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 65 (delta 1), reused 0 (delta 0), pack-reused 59\u001b[K\n",
            "Unpacking objects: 100% (65/65), 7.81 MiB | 11.76 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "base config file is downloaded from https://spacy.io/usage/training/#quickstart"
      ],
      "metadata": {
        "id": "-8zQo-1tGX-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = json.load(open('/content/spacyResumeParcer/data/training/train_data.json'))"
      ],
      "metadata": {
        "id": "l1Hpi0yGm9YB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cv_data)"
      ],
      "metadata": {
        "id": "7Ndv0d5inEm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de1022e-2faa-4937-f996-3bd2997dc6f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy init fill-config /content/spacyResumeParcer/data/training/base_config.cfg /content/spacyResumeParcer/data/training/config.cfg"
      ],
      "metadata": {
        "id": "W6E_W15XnTZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d96e00c-431a-47c4-cb75-bb1af2e7db20"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-06 12:53:19.154846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
            "/content/spacyResumeParcer/data/training/config.cfg\n",
            "You can now add your data and train your pipeline:\n",
            "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import e\n",
        "def get_spacy_doc (file, data):\n",
        "  nlp = spacy.blank('en')\n",
        "  db = DocBin()\n",
        "\n",
        "  for text, annot in tqdm (data):\n",
        "    doc = nlp.make_doc(text) \n",
        "    annot =  annot['entities']\n",
        "    ents = []\n",
        "    entity_indices = []\n",
        "    for start, end, label in annot:\n",
        "      skip_entity = False\n",
        "      for idx in range(start,end):\n",
        "        if idx in entity_indices:\n",
        "          skip_entity = True\n",
        "          break\n",
        "      if skip_entity == True:\n",
        "        continue\n",
        "\n",
        "      entity_indices = entity_indices + list(range(start,end))\n",
        "\n",
        "      try:\n",
        "        span = doc.char_span(start, end, label = label, alignment_mode = 'strict')\n",
        "      except:\n",
        "        continue\n",
        "      \n",
        "      if span is None:\n",
        "        err_data = str([start,end]) + \"    \" + str(text) + \"\\n\"\n",
        "      else:\n",
        "        ents.append(span)\n",
        "\n",
        "    try:\n",
        "      doc.ents = ents\n",
        "      db.add(doc)\n",
        "    except:\n",
        "      pass\n",
        "      \n",
        "  return db"
      ],
      "metadata": {
        "id": "WM6eTavbqi3n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test = train_test_split(cv_data,test_size = 0.3)\n",
        "len(train),len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g3TnqIYv4Rw",
        "outputId": "e3e4dfe2-845a-4ba5-c5df-b5812cae4aba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 60)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file  = open('error.txt','w')\n",
        "db = get_spacy_doc(file,train)\n",
        "db.to_disk('train_data.spacy')\n",
        "\n",
        "db = get_spacy_doc(file,test)\n",
        "db.to_disk('test_data.spacy')\n",
        "file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CWs4MbawTdZ",
        "outputId": "3d8cb544-3f58-437b-8c17-448214f5e479"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:01<00:00, 107.34it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 102.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy train /content/spacyResumeParcer/data/training/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pyoSObxwyb3",
        "outputId": "fab92eb9-24ff-4199-972c-140a378cbadf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-06 12:53:37.079910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;2m✔ Created output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
            "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
            "\u001b[1m\n",
            "=========================== Initializing pipeline ===========================\u001b[0m\n",
            "[2023-04-06 12:53:47,623] [INFO] Set up nlp object from config\n",
            "[2023-04-06 12:53:47,634] [INFO] Pipeline: ['transformer', 'ner']\n",
            "[2023-04-06 12:53:47,637] [INFO] Created vocabulary\n",
            "[2023-04-06 12:53:47,638] [INFO] Finished initializing nlp object\n",
            "Downloading (…)lve/main/config.json: 100% 481/481 [00:00<00:00, 85.9kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 14.1MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 7.00MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 18.4MB/s]\n",
            "Downloading pytorch_model.bin: 100% 501M/501M [00:02<00:00, 206MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-04-06 12:54:22,254] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
            "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
            "\u001b[1m\n",
            "============================= Training pipeline =============================\u001b[0m\n",
            "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
            "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
            "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
            "---  ------  -------------  --------  ------  ------  ------  ------\n",
            "  0       0        2343.80   1469.48    0.40    0.21    7.17    0.00\n",
            "  3     200      105129.20  63788.83   36.47   46.72   29.91    0.36\n",
            "  7     400       18726.81  21431.66   51.90   54.21   49.78    0.52\n",
            " 11     600        4821.96  18792.85   50.67   45.99   56.40    0.51\n",
            "\n",
            "\u001b[31mAborted.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Model"
      ],
      "metadata": {
        "id": "_KFPmvoIyQdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy-transformers"
      ],
      "metadata": {
        "id": "lflLGJic0K61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9936cf-0d85-4242-89ba-fc6b24e3ce5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy-transformers in /usr/local/lib/python3.9/dist-packages (1.2.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (2.4.6)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (3.5.1)\n",
            "Requirement already satisfied: transformers<4.27.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (4.26.1)\n",
            "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.27.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (23.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.1.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (67.6.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.10.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<4.0.0,>=3.5.0->spacy-transformers) (1.0.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers) (3.10.7)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers) (3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.8.0->spacy-transformers) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers) (3.25.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (0.13.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers) (6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy-transformers) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy-transformers) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy-transformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.8.0->spacy-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('/content/output/model-best')"
      ],
      "metadata": {
        "id": "3jAPWxv5yO8Q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "xKATeQLMDqpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d322d535-a3f7-4242-b330-1766c3c33195"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3977 sha256=0e26699d6e3490b0f44084d96c6ed8cf483829569840121c9359092dcdddaa95\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/75/01/e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt     # to convert docx files into text\n",
        "import PyPDF2       # to convert pdf files into text"
      ],
      "metadata": {
        "id": "E-UMddf-DpIb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing a single Resume"
      ],
      "metadata": {
        "id": "gtsq2l54W-F3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = ''\n",
        "fileName = '/content/spacyResumeParcer/inputFolder/1.pdf'\n",
        "try:\n",
        "    resume = docx2txt.process(fileName)\n",
        "    for i in resume:\n",
        "            if i == '\\n':\n",
        "                text += ' '\n",
        "            else:\n",
        "                text+=i\n",
        "except:\n",
        "    file = open(fileName,'rb')\n",
        "    reader=PyPDF2.PdfReader(file)\n",
        "    for pg in range(len(reader.pages)):\n",
        "        page = reader.pages[pg]\n",
        "        tex = page.extract_text()\n",
        "        for i in tex:\n",
        "            if type(i)==str and i == '\\n':\n",
        "                text += ' '\n",
        "            elif type(i)==str:\n",
        "                text+=i\n",
        "    file.close()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "3k4DIc_NEAoe",
        "outputId": "c26e4b84-330f-423a-955f-5a1eab254812"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Personal  InformationCURRICULUM VITAE FullNames: Mike KisasatiWanaswa IDCardNo.22859930 PostalAddress: P.O. Box 85575 80100 Mombasa TelephoneNo.0717 550926 EmailAddress:mikewanaswa@gmail.co mLanguages : Well spoken English and  SwahiliPurpose To put in use the latest inventions in Telecommunication and Information Technology for a  positive impact in Individuals, Business Enterprises and Corporate Organizations. Work Experience Date :April 2011 – To Date Position :Fixed Data Network Technician Employer :Ben’s Electronics Services Ltd, Mombasa Duties : Survey, Installation, Integration, Maintenance, Support and  Decommissioning of Fixed Data Services using various Access Technologies (WIMAX, FIBER, MICROWAVES and Wi-Fi) for  SafaricomLtd. : Survey, Installation and Support of Ceragon’s IP20 Access Technology for  Airtel (K) : Survey , Installation and Support of Cambridge P2MP Solutions for  Safaricom. : Fiber Optics Splicing and terminations, deployment, support and  maintenance : WAN/LAN Design, installation and Technical Support (Structured Cabling) : Installation and support for CCTV, IP Cameras, Biometrics Security Controls. : Installation and Support for RADWIN P2P Links and Ceragon PTMP,Links. : Installation and support for Telrad Wimax BTS : WI-FI setup, maintenance and Support. : E1 and SIP/VoIP installation and support : Systems Integration on Cisco, HP and Huawei Platforms for  MPLS, P2P,VPN and Internet. : Fixed LTE installation for Safaricom’s Enterprise clients. Date :September 2010 - April 2011 Position :Freelance Computer Technician Employer : Self Employed in Mombasa Duties : Installation of software and hardware for PCs, Servers, Printers. : Web design and website maintenance. : Repair of PCs, Printers, Scanners and other computer accessories : LAN/WAN design, installation and support. Date :March 2010 – August 2010 Position :Scanning Officer/System Administrator Employer : Interim Independent Electoral Commission Duties : Scanning of OMR Forms to extract data for the voters : Creation and maintenance of Voters’ database/Register : End User training and support on Voter Management System : MMC Administration/ Mail servers administration. : Network Management & Data Recovery : Repair and maintenance of Scanner, printers and Computers. Date : January 2009 - March 2010 Position :Freelance Computer T e c h n i c i a n Duties : Installation of software and hardware for PCs, Servers, Printers. : Repair of PCs, Printers, Scanners and other computer accessories : LAN/WAN Support. Educational Background Date : September 2007 – December 2008 Institution : Inoorero University ( Kenya School of Professional  studies) - NairobiCourse: Diploma in Computer Engineering, System  Administration and Support Grade:Credit Date : Feb 1997 – November 2000 Institution : Bungoma High School, Bungoma Grade :KCSE Mean Grade B- ( M i n u s ) Date : Jan 1989 – December 1996 Institution : Bungoma DEB Primary School, Bungoma Grade :KCPE 444 out of 700 Marks Professional Trainings Date : July2018 Institution : AmiranCommunications Course : Radwin Installer , P2P, P2MP Grade :Distinction Date : March2018 Institution : Cambridge Broadband Networks Kenya Ltd Course : VectarStarInstallation. Grade :Distinction Date : Jan2018 Institution : CeragonNetworks Course : Ceragon Certified Rollout Professional IP20C/S/E/N Grade :Distinction Date : June2017 Institution : E Learning -Ruckus Course : Ruckus Wireless Certified Associate – Support Engineer (RWCA-SupE) Grade :Distinction Date : Jan 2017 to Mar 2017 Institution : IATMombasa Course :CCNA Grade :DistinctionDate : July2015 Institution : Viscar Industrial Capacity Ltd Course : Fall Arrest Technician/Work at Height Grade :Distinction Note: Am in possession of a clean Class BCE Driving license Referees Patrick Odame Regional Election Coordinator, Bungoma P.O. Box 2568, 50200,  Bungoma Phone No. 0720  255  059snyodame@yahoo.co.u k Cyrus Soi Projects Manager, Bens Electronics P. O. Box 726 , 80100  Mombasa Phone  0710466547info@bensele ctronics.co.ke Christine Owuor Fixed Access Engineer,  Safaricom P O. Box 66827  00800 Nairobi Phone No. 0724 360 530 cowuor@safaricom.co.ke Kelvin Ongoro Field Engineer,Safaricom. P.O. Box 66827 00800  Nairobi Phone 0724619  217kongoro@safaricom .co.ke'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying Texts and Labels of Entities\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text,\"  :  \",ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXkfgZWg0nQy",
        "outputId": "7b2821f7-b67b-4eb0-d17b-15de6da76615"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mike KisasatiWanaswa   :   Name\n",
            "Mombasa   :   Location\n",
            "Cisco   :   Companies worked at\n",
            "Inoorero University   :   College Name\n",
            "Bungoma   :   Location\n",
            "Bungoma   :   Location\n",
            "Bungoma   :   Location\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading and Analyzing Bulk Resume(s)"
      ],
      "metadata": {
        "id": "AZAApTvgNBrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keywords to search in the resume\n",
        "keyString = \"C++,C,Python,Java,Excel,MatLab\"\n",
        "keylist = keyString.split(',')\n",
        "keylen = len(keylist)"
      ],
      "metadata": {
        "id": "Stdi0zIxOz9R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os  \n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "pathinp = r'/content/spacyResumeParcer/inputFolder'\n",
        "pathparent = r'..'\n",
        "os.chdir(pathinp)\n",
        "\n",
        "fileList = os.listdir()\n",
        "\n",
        "fileScore = []\n",
        "degree = []\n",
        "fileLen = len(fileList)\n",
        "nameList = []\n",
        "foundKeywordList = []\n",
        "emailList = []\n",
        "files = 0\n",
        "indicator = 0\n",
        "for fileName in fileList:\n",
        "\n",
        "    # Printing percentage of files processed\n",
        "    files+=1\n",
        "    if files%(fileLen//4) == 0:\n",
        "        indicator+=25\n",
        "        print(\"Processed {}%\".format(indicator))\n",
        "\n",
        "    # Reading text from a file into an array\n",
        "    keyfound = 0\n",
        "    text=''\n",
        "    try:\n",
        "        resume = docx2txt.process(fileName)\n",
        "        for i in resume:\n",
        "                if i == '\\n':\n",
        "                    text += ' '\n",
        "                else:\n",
        "                    text+=i\n",
        "    except:\n",
        "        file = open(fileName,'rb')\n",
        "        reader=PyPDF2.PdfReader(file)\n",
        "        for pg in range(len(reader.pages)):\n",
        "            page = reader.pages[pg]\n",
        "            tex = page.extract_text()\n",
        "            for i in tex:\n",
        "                if type(i)==str and i == '\\n':\n",
        "                    text += ' '\n",
        "                elif type(i)==str:\n",
        "                    text+=i\n",
        "        file.close()\n",
        "        \n",
        "    textRaw = text\n",
        "    textOriginal = text.split(' ')\n",
        "\n",
        "    # Searching for email\n",
        "    regex = r\"([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+\\.[a-zA-Z0-9_-]+) | ([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9_-]+)\"\n",
        "    matches = re.search(regex, text)\n",
        "    canEmail = ''\n",
        "    try:\n",
        "        canEmail = str((matches.group()).strip())\n",
        "    except:\n",
        "        canEmail = \"<Not Found>\"\n",
        "    emailList.append(canEmail)\n",
        "\n",
        "    # Searching Names\n",
        "    doc = nlp(text)\n",
        "    name = '<Not Found>'\n",
        "    for ent in doc.ents:\n",
        "      if str(ent.label_) == \"Name\":\n",
        "        name = str(ent.text)\n",
        "        break\n",
        "    nameList.append(name)\n",
        "\n",
        "    # Searching required keywords\n",
        "    found_skills = ''\n",
        "    for ent in doc.ents:\n",
        "      if str(ent.label_) == \"Skills\":\n",
        "        found_skills = str(ent.text)\n",
        "    foundKeywords=''\n",
        "    for ky in keylist:\n",
        "        if found_skills.find(ky) != -1:\n",
        "            keyfound += 1\n",
        "            foundKeywords += ky +', '\n",
        "    foundKeywordList.append(foundKeywords)\n",
        "    fileScore.append(keyfound*100/keylen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOgFdpYCNI0y",
        "outputId": "8401c096-519a-4b10-b973-e2b2ff73088e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (694 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a dataframe and exporting it to a csv\n",
        "os.chdir(pathparent)\n",
        "df = pd.DataFrame({\"File Names\" : fileList, r\"% Keywords Matched\" : fileScore,\"Keywords Found\":foundKeywordList,\"Name\":nameList,\"Email\":emailList})\n",
        "df=df[df[r\"% Keywords Matched\"]!=0]\n",
        "df = df.sort_values(r\"% Keywords Matched\",ascending=False)\n",
        "print(df)\n",
        "df.to_csv(\"/content/spacyResumeParcer/resumeResult.csv\", index=False)\n",
        "print(\"---  %s files scanned ---\" % (fileLen))"
      ],
      "metadata": {
        "id": "wVbNEJo4To2q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}